#!/usr/bin/env python3
"""
å¤„ç†æ‰€æœ‰DOCXæ–‡ä»¶å¹¶åˆ›å»ºå‘é‡åº“
æ¯æœ¬ä¹¦åˆ›å»ºä¸€ä¸ªç‹¬ç«‹çš„collection
"""
import sys
import os
import re
from pathlib import Path
from loguru import logger

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from src.preprocessing.docx_parser import DOCXParser
from src.preprocessing.vectorstore_builder import VectorStoreBuilder

def setup_logging():
    """è®¾ç½®æ—¥å¿—"""
    logger.remove()
    logger.add(
        sys.stdout,
        format="<green>{time:HH:mm:ss}</green> | <level>{level: <8}</level> | <level>{message}</level>",
        level="INFO"
    )

def clean_collection_name(filename: str) -> str:
    """æ¸…ç†æ–‡ä»¶åï¼Œç”Ÿæˆåˆæ³•çš„collectionåç§°"""
    # ç§»é™¤æ‰©å±•å
    name = Path(filename).stem
    
    # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œåªä¿ç•™å­—æ¯ã€æ•°å­—ã€ä¸‹åˆ’çº¿å’Œä¸­æ–‡
    name = re.sub(r'[^\w\u4e00-\u9fff]', '_', name)
    
    # ç§»é™¤è¿ç»­çš„ä¸‹åˆ’çº¿
    name = re.sub(r'_+', '_', name)
    
    # ç§»é™¤å¼€å¤´å’Œç»“å°¾çš„ä¸‹åˆ’çº¿
    name = name.strip('_')
    
    # é™åˆ¶é•¿åº¦
    if len(name) > 50:
        name = name[:50]
    
    # æ·»åŠ å‰ç¼€
    collection_name = f"docx_{name}"
    
    return collection_name

def process_all_docx():
    """å¤„ç†æ‰€æœ‰DOCXæ–‡ä»¶"""
    setup_logging()
    
    logger.info("=== å¼€å§‹å¤„ç†DOCXæ–‡ä»¶å¹¶åˆ›å»ºå‘é‡åº“ ===")
    
    # æŸ¥æ‰¾æ‰€æœ‰DOCXæ–‡ä»¶
    data_dir = Path("data")
    docx_files = list(data_dir.glob("*.docx"))
    
    if not docx_files:
        logger.error("âŒ æœªæ‰¾åˆ°DOCXæ–‡ä»¶")
        return
    
    logger.info(f"ğŸ“š æ‰¾åˆ° {len(docx_files)} ä¸ªDOCXæ–‡ä»¶")
    for i, file in enumerate(docx_files, 1):
        logger.info(f"  {i}. {file.name} ({file.stat().st_size / 1024 / 1024:.1f}MB)")
    
    # åˆ›å»ºè§£æå™¨å’Œå‘é‡å­˜å‚¨æ„å»ºå™¨
    parser = DOCXParser()
    builder = VectorStoreBuilder()
    
    # å¤„ç†æ¯ä¸ªæ–‡ä»¶
    all_chunks = {}
    
    for i, docx_file in enumerate(docx_files, 1):
        logger.info(f"\nğŸ“– å¤„ç†æ–‡ä»¶ {i}/{len(docx_files)}: {docx_file.name}")
        
        try:
            # è§£æDOCXæ–‡ä»¶
            logger.info("  â³ æ­£åœ¨è§£æDOCX...")
            chunks = parser.parse_docx(str(docx_file))
            
            if not chunks:
                logger.warning(f"  âš ï¸  {docx_file.name} è§£æç»“æœä¸ºç©ºï¼Œè·³è¿‡")
                continue
            
            logger.info(f"  âœ“ è§£æå®Œæˆï¼Œå…± {len(chunks)} ä¸ªæ–‡æœ¬å—")
            
            # æ·»åŠ å…ƒæ•°æ®
            book_id = f"book_{i}"
            book_name = docx_file.stem
            
            for chunk in chunks:
                chunk["metadata"]["book_id"] = book_id
                chunk["metadata"]["book_name"] = book_name
                chunk["metadata"]["version"] = "1"
                chunk["metadata"]["filename"] = docx_file.name
                chunk["metadata"]["file_type"] = "docx"
            
            # ç”Ÿæˆcollectionåç§°
            collection_name = clean_collection_name(docx_file.name)
            logger.info(f"  ğŸ“¦ Collectionåç§°: {collection_name}")
            
            # ä¿å­˜åˆ°å­—å…¸
            all_chunks[collection_name] = chunks
            
            logger.info(f"  âœ… {docx_file.name} å¤„ç†å®Œæˆ")
            
        except Exception as e:
            logger.error(f"  âŒ å¤„ç†å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            continue
    
    if not all_chunks:
        logger.error("âŒ æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•æ–‡æ¡£")
        return
    
    # æ„å»ºå‘é‡æ•°æ®åº“
    logger.info(f"\nğŸ”¨ å¼€å§‹æ„å»ºå‘é‡æ•°æ®åº“...")
    logger.info(f"   æ€»å…± {len(all_chunks)} ä¸ªcollections")
    logger.info(f"   æ€»å…± {sum(len(chunks) for chunks in all_chunks.values())} ä¸ªæ–‡æœ¬å—")
    
    try:
        builder.build_all_collections(all_chunks, force_rebuild=True)
        logger.info("âœ… å‘é‡æ•°æ®åº“æ„å»ºå®Œæˆ")
        
        # æ˜¾ç¤ºç»“æœ
        logger.info("\nğŸ“Š å¤„ç†ç»“æœæ±‡æ€»:")
        for collection_name, chunks in all_chunks.items():
            logger.info(f"  âœ“ {collection_name}: {len(chunks)} ä¸ªæ–‡æœ¬å—")
        
        # ä¿å­˜å¤„ç†æ‘˜è¦
        import json
        summary = {
            "total_collections": len(all_chunks),
            "total_chunks": sum(len(chunks) for chunks in all_chunks.values()),
            "collections": {}
        }
        
        for collection_name, chunks in all_chunks.items():
            summary["collections"][collection_name] = {
                "chunk_count": len(chunks),
                "file_type": "docx",
                "source_file": chunks[0]["metadata"]["filename"] if chunks else "unknown"
            }
        
        output_dir = Path("data/processed")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        with open(output_dir / "docx_processing_summary.json", 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ“ å¤„ç†æ‘˜è¦å·²ä¿å­˜åˆ°: {output_dir / 'docx_processing_summary.json'}")
        
        logger.info("\nğŸ‰ æ‰€æœ‰æ–‡æ¡£å¤„ç†å®Œæˆï¼")
        logger.info("ğŸ’¡ ç°åœ¨å¯ä»¥å¯åŠ¨åç«¯APIè¿›è¡Œé—®ç­”äº†")
        
    except Exception as e:
        logger.error(f"âŒ å‘é‡æ•°æ®åº“æ„å»ºå¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())

if __name__ == "__main__":
    process_all_docx()
