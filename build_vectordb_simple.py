#!/usr/bin/env python3
"""
ç®€å•ç›´æ¥çš„å‘é‡åº“æ„å»ºè„šæœ¬
ç›´æ¥ä½¿ç”¨ChromaDBå’Œsentence-transformersï¼Œä¸ä¾èµ–langchain
"""
import sys
import re
from pathlib import Path
from loguru import logger
import chromadb
from sentence_transformers import SentenceTransformer

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from src.preprocessing.docx_parser import DOCXParser

def setup_logging():
    """è®¾ç½®æ—¥å¿—"""
    logger.remove()
    logger.add(
        sys.stdout,
        format="<green>{time:HH:mm:ss}</green> | <level>{level: <8}</level> | <level>{message}</level>",
        level="INFO"
    )

def clean_collection_name(filename: str) -> str:
    """æ¸…ç†æ–‡ä»¶åï¼Œç”Ÿæˆåˆæ³•çš„collectionåç§°"""
    name = Path(filename).stem
    # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
    name = re.sub(r'[^\w\u4e00-\u9fff]', '_', name)
    name = re.sub(r'_+', '_', name).strip('_')
    # é™åˆ¶é•¿åº¦
    name = name[:50] if len(name) > 50 else name
    return f"docx_{name}"

def main():
    setup_logging()
    
    logger.info("=== æ„å»ºå‘é‡æ•°æ®åº“ ===")
    
    # 1. åˆå§‹åŒ–ChromaDB
    logger.info("ğŸ“¦ åˆå§‹åŒ–ChromaDB...")
    chroma_path = "./database/chroma_db"
    client = chromadb.PersistentClient(path=chroma_path)
    logger.info(f"âœ“ ChromaDBè·¯å¾„: {chroma_path}")
    
    # 2. åˆå§‹åŒ–Embeddingæ¨¡å‹
    logger.info("ğŸ¤– åŠ è½½Embeddingæ¨¡å‹...")
    try:
        # ä½¿ç”¨è½»é‡çº§ä¸­æ–‡æ¨¡å‹
        model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
        logger.info("âœ“ Embeddingæ¨¡å‹åŠ è½½æˆåŠŸ")
    except Exception as e:
        logger.error(f"âŒ Embeddingæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return
    
    # 3. æŸ¥æ‰¾æ‰€æœ‰DOCXæ–‡ä»¶
    logger.info("ğŸ“š æŸ¥æ‰¾DOCXæ–‡ä»¶...")
    data_dir = Path("data")
    docx_files = list(data_dir.glob("*.docx"))
    
    if not docx_files:
        logger.error("âŒ æœªæ‰¾åˆ°DOCXæ–‡ä»¶")
        return
    
    logger.info(f"æ‰¾åˆ° {len(docx_files)} ä¸ªDOCXæ–‡ä»¶:")
    for i, file in enumerate(docx_files, 1):
        logger.info(f"  {i}. {file.name}")
    
    # 4. å¤„ç†æ¯ä¸ªDOCXæ–‡ä»¶
    parser = DOCXParser()
    
    for i, docx_file in enumerate(docx_files, 1):
        logger.info(f"\nğŸ“– [{i}/{len(docx_files)}] å¤„ç†: {docx_file.name}")
        
        try:
            # ç”Ÿæˆcollectionåç§°
            collection_name = clean_collection_name(docx_file.name)
            logger.info(f"  Collection: {collection_name}")
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            try:
                existing = client.get_collection(collection_name)
                logger.info(f"  âš ï¸  Collectionå·²å­˜åœ¨ï¼Œåˆ é™¤é‡å»º...")
                client.delete_collection(collection_name)
            except:
                pass
            
            # åˆ›å»ºcollection
            collection = client.create_collection(
                name=collection_name,
                metadata={"source": docx_file.name}
            )
            
            # è§£æDOCX
            logger.info("  â³ è§£æDOCX...")
            chunks = parser.parse_docx(str(docx_file))
            
            if not chunks:
                logger.warning("  âš ï¸  è§£æç»“æœä¸ºç©ºï¼Œè·³è¿‡")
                continue
            
            logger.info(f"  âœ“ è§£æå®Œæˆ: {len(chunks)} ä¸ªæ–‡æœ¬å—")
            
            # å‡†å¤‡æ•°æ®
            logger.info("  â³ ç”Ÿæˆå‘é‡...")
            documents = []
            metadatas = []
            ids = []
            
            for j, chunk in enumerate(chunks):
                documents.append(chunk["content"])
                metadatas.append({
                    "book_name": docx_file.stem,
                    "filename": docx_file.name,
                    "chapter": chunk["metadata"].get("chapter", "æœªçŸ¥ç« èŠ‚"),
                    "chunk_id": j
                })
                ids.append(f"{collection_name}_{j}")
            
            # æ‰¹é‡å¤„ç†ï¼Œæ¯æ¬¡100ä¸ª
            batch_size = 100
            for start_idx in range(0, len(documents), batch_size):
                end_idx = min(start_idx + batch_size, len(documents))
                batch_docs = documents[start_idx:end_idx]
                batch_metas = metadatas[start_idx:end_idx]
                batch_ids = ids[start_idx:end_idx]
                
                # ç”Ÿæˆembeddings
                embeddings = model.encode(batch_docs).tolist()
                
                # æ·»åŠ åˆ°collection
                collection.add(
                    embeddings=embeddings,
                    documents=batch_docs,
                    metadatas=batch_metas,
                    ids=batch_ids
                )
                
                logger.info(f"  âœ“ å·²å¤„ç† {end_idx}/{len(documents)} ä¸ªæ–‡æœ¬å—")
            
            logger.info(f"  âœ… {docx_file.name} å¤„ç†å®Œæˆ")
            
        except Exception as e:
            logger.error(f"  âŒ å¤„ç†å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            continue
    
    # 5. æ˜¾ç¤ºç»“æœ
    logger.info("\nğŸ“Š å‘é‡åº“æ„å»ºå®Œæˆï¼")
    logger.info("å¯ç”¨çš„Collections:")
    collections = client.list_collections()
    for coll in collections:
        count = coll.count()
        logger.info(f"  âœ“ {coll.name}: {count} ä¸ªæ–‡æ¡£")
    
    logger.info("\nğŸ‰ å…¨éƒ¨å®Œæˆï¼ç°åœ¨å¯ä»¥å¯åŠ¨åç«¯è¿›è¡Œé—®ç­”äº†")

if __name__ == "__main__":
    main()
